---
title: "ci1_ca2"
author: "Gaelan Gu"
date: "04/05/2017"
output:
  md_document:
    variant: markdown_github
---

# KE5206 | Computational Intelligence I CA2
----------------------------------------------------------------------------
*Support Vector Machines*


## Data & Package Imports
```{r}
library(ISLR)
library(e1071)
library(caret)
library(caTools)
data(Default)
summary(Default)
```

We import the necessary packages in order to complete this assignment. The *Default* dataset is taken from the *ISLR* package. We will also deploy SVM from the *e1071* package, to train our model.

*caret* and *caTools* are essential packages used to perform various data processes.

From the summary of *Default*, there are 3 independent variables (binary variable: *student*, numerical variables *balance* and *income*) and one dependent variable (*default*). We will build a model to predict the *default* variable.


## Train-Test Split
We randomly pick 80% of the dataset to be the training set, and the rest as the validation set.
```{r}
set.seed(111)
split = sample.split(Default$default, SplitRatio = 0.8)

train = subset(Default, split == T)
test = subset(Default, split == F)

# train and test sets with balance and income only
train_bi = train[, c('default', 'balance', 'income')]
test_bi = test[, c('default', 'balance', 'income')]

# train and test sets with student and balance only
train_bs = train[, c('default', 'balance', 'student')]
test_bs = test[, c('default', 'balance', 'student')]
```

In addition, we have created subsets of the training and test sets by only selecting 2 variables each. One subset will have *balance* and *income* as the predictors (BI dataset), while the other subset will have *balance* and *student* (BS dataset).

These subsets will be trained on SVM using both radial and sigmoid kernels. These models will also be tuned to determine the best parameters (cost and gamma) to be used, to optimize training.

## Balance & Income Dataset
### Fitting Train Set with SVM with *Radial* Kernel
We first work on the dataset with the *balance* and *income* variables. We are ignoring the *student* variable in this case.
```{r}
svm_bi1 = svm(default ~ .,
           data = train_bi,
           kernel = 'radial',
           gamma = 1,
           cost = 1)

summary(svm_bi1)
```

### Tuning of SVM kernel
```{r}
set.seed(111)

tune_out_bi1 = tune(svm,
                 default ~ .,
                 data = train_bi,
                 kernel = 'radial',
                 ranges = list(cost = c(0.1, 1, 10, 100, 1000),
                               gamma = c(0.5, 1, 2, 3, 4)))

summary(tune_out_bi1)
```

After tuning, we discover that the best parameters are cost = 100 and gamma = 1.

```{r}
bestmod_bi1 = tune_out_bi1$best.model
summary(bestmod_bi1)
```

Number of support vectors have reduced in this case, when cost increased and gamma remains unchanged. This would imply that the margin has reduced.


### Confusion Matrices
```{r}
# CM on Training Set
newpred_bi_train1 = predict(bestmod_bi1, train_bi)
confusionMatrix(table(prediction = newpred_bi_train1,
                      actual = train_bi$default))
```

**97.4%** accuracy on training set achieved, which is pretty good.

```{r}
# CM on Test Set
newpred_bi_test1 = predict(bestmod_bi1, test_bi)
confusionMatrix(table(prediction = newpred_bi_test1,
                      actual = test_bi$default))
```

**97.1%** accuracy on test set achieved.

### Fitting Train Set with SVM with *Sigmoid* Kernel
We now build a SVM with a sigmoid kernel to determine if it is more effective than one with a radial basis kernel.
```{r}
svm_bi2 = svm(default ~ .,
           data = train_bi,
           kernel = 'sigmoid',
           gamma = 1,
           cost = 1)

summary(svm_bi2)
```


### Tuning of SVM Kernel
```{r}
set.seed(111)

tune_out_bi2 = tune(svm,
                 default ~ .,
                 data = train_bi,
                 kernel = 'sigmoid',
                 ranges = list(cost = c(0.1, 1, 10, 100, 1000),
                               gamma = c(0.5, 1, 2, 3, 4)))

summary(tune_out_bi2)
```

After tuning, the best parameters are cost = 0.1 and gamma = 0.5.

```{r}
bestmod_bi2 = tune_out_bi2$best.model
summary(bestmod_bi2)
```

Both cost and gamma parameters have been reduced after tuning, and number of support vectors has increased. There is now a larger margin due to a smaller cost and gamma.


### Confusion Matrices
```{r}
# CM on Training Set
newpred_bi_train2 = predict(bestmod_bi2, train_bi)
confusionMatrix(table(prediction = newpred_bi_train2,
                      actual = train_bi$default))
```

Accuracy rate of **95.5%** achieved on training set.

```{r}
# CM on Test Set
newpred_bi_test2 = predict(bestmod_bi2, test_bi)
confusionMatrix(table(prediction = newpred_bi_test2,
                      actual = test_bi$default))
```

Accuracy of **94.8%** achieved on test set.

For this dataset, it is apparent that the SVM with a radial basis kernel has a better performance of 97.1% accuracy.


## Balance & Student Dataset
### Fitting Train Set with SVM with *Radial* Kernel
We now consider the dataset with only the *balance* and *student* variables.
```{r}
svm_bs1 = svm(default ~ .,
           data = train_bs,
           kernel = 'radial',
           gamma = 1,
           cost = 1)

summary(svm_bs1)
```


### Tuning of SVM Kernel
```{r}
set.seed(111)

tune_out_bs1 = tune(svm,
                 default ~ .,
                 data = train_bs,
                 kernel = 'radial',
                 ranges = list(cost = c(0.1, 1, 10, 100, 1000),
                               gamma = c(0.5, 1, 2, 3, 4)))

summary(tune_out_bs1)
```

Best parameters are cost = 10 and gamma = 3.

```{r}
bestmod_bs1 = tune_out_bs1$best.model
summary(bestmod_bs1)
```

Number of support vectors have increased tremendously after using a moderate cost and high gamma. There is a much larger margin in this SVM.


### Confusion Matrices
```{r}
# CM on Training Set
newpred_bs_train1 = predict(bestmod_bs1, train_bs)
confusionMatrix(table(prediction = newpred_bs_train1,
                      actual = train_bs$default))
```

Accuracy of **97.4%** achieved on training set.

```{r}
# CM on Test Set
newpred_bs_test1 = predict(bestmod_bs1, test_bs)
confusionMatrix(table(prediction = newpred_bs_test1,
                      actual = test_bs$default))
```

**97.2%** achieved on test set, which is very close to the performance of the radial basis kernel using the *balance* and *income* dataset.


### Fitting Train Set with SVM with *Sigmoid* Kernel
```{r}
svm_bs2 = svm(default ~ .,
           data = train_bs,
           kernel = 'sigmoid',
           gamma = 1,
           cost = 1)

summary(svm_bs2)
```


### Tuning of SVM Kernel
```{r}
set.seed(111)

tune_out_bs2 = tune(svm,
                 default ~ .,
                 data = train_bs,
                 kernel = 'sigmoid',
                 ranges = list(cost = c(0.1, 1, 10, 100, 1000),
                               gamma = c(0.5, 1, 2, 3, 4)))

summary(tune_out_bs2)
```

Best parameters are cost = 0.1 and gamma = 2.

```{r}
bestmod_bs2 = tune_out_bs2$best.model
summary(bestmod_bs2)
```

With cost reduced and gamma slightly increased, we now have more support vectors with a larger margin.


### Confusion Matrices
```{r}
# CM on Training Set
newpred_bs_train2 = predict(bestmod_bs2, train_bs)
confusionMatrix(table(prediction = newpred_bs_train2,
                      actual = train_bs$default))
```

Accuracy of **96.9%** achieved on training set.

```{r}
# CM on Test Set
newpred_bs_test2 = predict(bestmod_bs2, test_bs)
confusionMatrix(table(prediction = newpred_bs_test2,
                      actual = test_bs$default))
```

**96.8%** achieved on test set.

The performance is not very different from that of the radial basis kernel, which suggests that using the *student* variable in analysis greatly helps to predict this binary classification problem.


## Conclusion
We compare the best parameters of the tuned models in the following table:

| Best Params | BI (Radial) | BI (Sigmoid) | BS (Radial) | BS (Sigmoid) |
|-------------|-------------|--------------|-------------|--------------|
|        Cost |         100 |          0.1 |      **10** |          0.1 |
|       Gamma |           1 |          0.5 |       **3** |            2 |
|  No. of SVs |         540 |          521 |     **837** |          533 |

We are aware that the smaller the cost, the larger the margin and the more support vectors there will be. As for gamma, the higher it is, the more it allows the SVM to capture the shape of the data but there might be a risk of overfitting. There is also a large margin in our best model, which would explain the large number of support vectors.


We summarize the performances of the SVM kernels in the table below:

| BI (Radial) | BI (Sigmoid) | BS (Radial) | BS (Sigmoid) |
|-------------|--------------|-------------|--------------|
|       97.1% |        94.8% |   **97.2%** |        96.8% |

The radial basis kernel has the better performance as compared to the sigmoid kernel. We also discovered that our accuracy rates are noticeably higher when using the BS dataset, which implies that the *student* variable is a more effective predictor than *income*.


