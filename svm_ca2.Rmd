---
title: "ci1_ca2"
author: "Gaelan Gu"
date: "04/05/2017"
output: word_document
---

# KE5206 | Computational Intelligence I CA2
----------------------------------------------------------------------------
*Support Vector Machines*


## Data & Package Imports
```{r}
library(ISLR)
library(e1071)
library(caret)
library(caTools)
data(Default)
summary(Default)
```

We import the necessary packages in order to complete this assignment. The *Default* dataset is taken from the *ISLR* package. We will also deploy SVM from the *e1071* package, to train our model.

*caret* and *caTools* are essential packages used to perform various data processes.

From the summary of *Default*, there are 3 independent variables (binary variable: *student*, numerical variables *balance* and *income*) and one dependent variable (*default*). We will build a model to predict the *default* variable.


## Train-Test Split
We randomly pick 80% of the dataset to be the training set, and the rest as the validation set.
```{r}
set.seed(111)
split = sample.split(Default$default, SplitRatio = 0.8)

train = subset(Default, split == T)
test = subset(Default, split == F)

# train and test sets with balance and income only
train_bi = train[, c('default', 'balance', 'income')]
test_bi = test[, c('default', 'balance', 'income')]

# train and test sets with student and balance only
train_bs = train[, c('default', 'balance', 'student')]
test_bs = test[, c('default', 'balance', 'student')]

# train and test sets with student and balance only
train_is = train[, c('default', 'income', 'student')]
test_is = test[, c('default', 'income', 'student')]
```

In addition, we have created subsets of the training and test sets by only selecting 2 variables each.

These subsets will be trained on SVM using both radial and sigmoid kernels. These models will also be tuned to determine the best parameters (cost and gamma) to be used, to optimize training.


## Balance & Income Dataset (BI)
### Fitting Train Set with SVM with *Radial* Kernel
```{r}
svm_bi1 = svm(default ~ .,
           data = train_bi,
           kernel = 'radial',
           gamma = 1,
           cost = 1)

summary(svm_bi1)
```

### Tuning of SVM kernel
```{r}
set.seed(111)

tune_out_bi1 = tune(svm,
                 default ~ .,
                 data = train_bi,
                 kernel = 'radial',
                 ranges = list(cost = c(0.1, 1, 10, 100, 1000),
                               gamma = c(0.5, 1, 2, 3, 4)),
                 tunecontrol = tune.control(sampling = 'cross',
                                            cross = 5))

summary(tune_out_bi1)
```

After tuning, we discover that the best parameters are cost = 100 and gamma = 0.5.


```{r}
bestmod_bi1 = tune_out_bi1$best.model
summary(bestmod_bi1)
```


### Confusion Matrix
```{r}
# CM on Test Set
newpred_bi_test1 = predict(bestmod_bi1, test_bi)
confusionMatrix(table(prediction = newpred_bi_test1,
                      actual = test_bi$default))
```

**97.1%** accuracy on test set achieved.


### Fitting Train Set with SVM with *Sigmoid* Kernel
```{r}
svm_bi2 = svm(default ~ .,
           data = train_bi,
           kernel = 'sigmoid',
           gamma = 1,
           cost = 1)

summary(svm_bi2)
```


### Tuning of SVM Kernel
```{r}
set.seed(111)

tune_out_bi2 = tune(svm,
                 default ~ .,
                 data = train_bi,
                 kernel = 'sigmoid',
                 ranges = list(cost = c(0.1, 1, 10, 100, 1000),
                               gamma = c(0.5, 1, 2, 3, 4)),
                 tunecontrol = tune.control(sampling = 'cross',
                                            cross = 5))

summary(tune_out_bi2)
```

After tuning, the best parameters are cost = 0.1 and gamma = 0.5.


```{r}
bestmod_bi2 = tune_out_bi2$best.model
summary(bestmod_bi2)
```


### Confusion Matrix
```{r}
# CM on Test Set
newpred_bi_test2 = predict(bestmod_bi2, test_bi)
confusionMatrix(table(prediction = newpred_bi_test2,
                      actual = test_bi$default))
```

Accuracy of **94.8%** achieved on test set.


## Balance & Student Dataset (BS)
### Fitting Train Set with SVM with *Radial* Kernel
```{r}
svm_bs1 = svm(default ~ .,
           data = train_bs,
           kernel = 'radial',
           gamma = 1,
           cost = 1)

summary(svm_bs1)
```


### Tuning of SVM Kernel
```{r}
set.seed(111)

tune_out_bs1 = tune(svm,
                 default ~ .,
                 data = train_bs,
                 kernel = 'radial',
                 ranges = list(cost = c(0.1, 1, 10, 100, 1000),
                               gamma = c(0.5, 1, 2, 3, 4)),
                 tunecontrol = tune.control(sampling = 'cross',
                                            cross = 5))

summary(tune_out_bs1)
```

Best parameters are cost = 1000 and gamma = 1.


```{r}
bestmod_bs1 = tune_out_bs1$best.model
summary(bestmod_bs1)
```


### Confusion Matrix
```{r}
# CM on Test Set
newpred_bs_test1 = predict(bestmod_bs1, test_bs)
confusionMatrix(table(prediction = newpred_bs_test1,
                      actual = test_bs$default))
```

Accuracy of **97.2%** achieved on test set.


### Fitting Train Set with SVM with *Sigmoid* Kernel
```{r}
svm_bs2 = svm(default ~ .,
           data = train_bs,
           kernel = 'sigmoid',
           gamma = 1,
           cost = 1)

summary(svm_bs2)
```


### Tuning of SVM Kernel
```{r}
set.seed(111)

tune_out_bs2 = tune(svm,
                 default ~ .,
                 data = train_bs,
                 kernel = 'sigmoid',
                 ranges = list(cost = c(0.1, 1, 10, 100, 1000),
                               gamma = c(0.5, 1, 2, 3, 4)),
                 tunecontrol = tune.control(sampling = 'cross',
                                            cross = 5))

summary(tune_out_bs2)
```

Best parameters are cost = 0.1 and gamma = 2.


```{r}
bestmod_bs2 = tune_out_bs2$best.model
summary(bestmod_bs2)
```


### Confusion Matrix
```{r}
# CM on Test Set
newpred_bs_test2 = predict(bestmod_bs2, test_bs)
confusionMatrix(table(prediction = newpred_bs_test2,
                      actual = test_bs$default))
```

Accuracy of **96.8%** achieved on test set.


## Income & Student Dataset (IS)
### Fitting Train Set with SVM with *Radial* Kernel
```{r}
svm_is1 = svm(default ~ .,
           data = train_is,
           kernel = 'radial',
           gamma = 1,
           cost = 1)

summary(svm_is1)
```

### Tuning of SVM Kernel
```{r}
set.seed(111)

tune_out_is1 = tune(svm,
                 default ~ .,
                 data = train_is,
                 kernel = 'radial',
                 ranges = list(cost = c(0.1, 1, 10, 100, 1000),
                               gamma = c(0.5, 1, 2, 3, 4)),
                 tunecontrol = tune.control(sampling = 'cross',
                                            cross = 5))

summary(tune_out_is1)
```

Best parameters are cost = 0.1 and gamma = 0.5.


```{r}
bestmod_is1 = tune_out_is1$best.model
summary(bestmod_is1)
```


### Confusion Matrix
```{r}
# CM on Test Set
newpred_is_test1 = predict(bestmod_is1, test_is)
confusionMatrix(table(prediction = newpred_is_test1,
                      actual = test_is$default))
```

Accuracy of **96.7%** achieved on test set.


### Fitting Train Set with SVM with *Sigmoid* Kernel
```{r}
svm_is2 = svm(default ~ .,
           data = train_is,
           kernel = 'sigmoid',
           gamma = 1,
           cost = 1)

summary(svm_is2)
```


### Tuning of SVM Kernel
```{r}
set.seed(111)

tune_out_is2 = tune(svm,
                 default ~ .,
                 data = train_is,
                 kernel = 'sigmoid',
                 ranges = list(cost = c(0.1, 1, 10, 100, 1000),
                               gamma = c(0.5, 1, 2, 3, 4)),
                 tunecontrol = tune.control(sampling = 'cross',
                                            cross = 5))

summary(tune_out_is2)
```

Best parameters are cost = 0.1 and gamma = 0.5.


```{r}
bestmod_is2 = tune_out_is2$best.model
summary(bestmod_is2)
```


### Confusion Matrix
```{r}
# CM on Test Set
newpred_is_test2 = predict(bestmod_is2, test_is)
confusionMatrix(table(prediction = newpred_is_test2,
                      actual = test_is$default))
```

Accuracy of **96.7%** achieved on test set.


## Full Training Dataset (ALL)
### Fitting Train Set with SVM with *Radial* Kernel
```{r}
svm_all1 = svm(default ~ .,
           data = train,
           kernel = 'radial',
           gamma = 1,
           cost = 1)

summary(svm_all1)
```


### Tuning of SVM Kernel
```{r}
set.seed(111)

tune_out_all1 = tune(svm,
                 default ~ .,
                 data = train,
                 kernel = 'radial',
                 ranges = list(cost = c(0.1, 1, 10, 100, 1000),
                               gamma = c(0.5, 1, 2, 3, 4)),
                 tunecontrol = tune.control(sampling = 'cross',
                                            cross = 5))

summary(tune_out_all1)
```

Best parameters are cost = 10 and gamma = 2.


```{r}
bestmod_all1 = tune_out_all1$best.model
summary(bestmod_all1)
```


### Confusion Matrix
```{r}
# CM on Test Set
newpred_all_test1 = predict(bestmod_all1, test)
confusionMatrix(table(prediction = newpred_all_test1,
                      actual = test$default))
```

Accuracy of **97.2%** achieved on test set.


### Fitting Train Set with SVM with *Sigmoid* Kernel
```{r}
svm_all2 = svm(default ~ .,
           data = train,
           kernel = 'sigmoid',
           gamma = 1,
           cost = 1)

summary(svm_all2)
```


### Tuning of SVM Kernel
```{r}
set.seed(111)

tune_out_all2 = tune(svm,
                 default ~ .,
                 data = train,
                 kernel = 'sigmoid',
                 ranges = list(cost = c(0.1, 1, 10, 100, 1000),
                               gamma = c(0.5, 1, 2, 3, 4)),
                 tunecontrol = tune.control(sampling = 'cross',
                                            cross = 5))

summary(tune_out_all2)
```

Best parameters are cost = 0.1 and gamma = 0.5.


```{r}
bestmod_all2 = tune_out_all2$best.model
summary(bestmod_all2)
```


### Confusion Matrix
```{r}
# CM on Test Set
newpred_all_test2 = predict(bestmod_all2, test)
confusionMatrix(table(prediction = newpred_all_test2,
                      actual = test$default))
```

Accuracy of **95.2%** achieved on test set.


## Conclusion
We compare the best parameters of the tuned models in the following table:





We are aware that the smaller the cost, the larger the margin and the more support vectors there will be. As for gamma, the higher it is, the more it allows the SVM to capture the shape of the data but there might be a risk of overfitting. There is also a large margin in our best model, which would explain the large number of support vectors.


We summarize the performances of the SVM kernels in the table below:




The radial basis kernel has the better performance as compared to the sigmoid kernel. We also discovered that our accuracy rates are noticeably higher when using the BS dataset, which implies that the *student* variable is a more effective predictor than *income*.


